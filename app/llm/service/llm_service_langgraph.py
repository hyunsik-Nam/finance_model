# import getpass
# import os
# from typing import Any, Dict, List, TypedDict, Literal
import json

# from dotenv import load_dotenv
# from langchain.chat_models import init_chat_model
# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain_core.callbacks import BaseCallbackHandler
# from langchain_core.messages import BaseMessage
# from langchain_core.outputs import LLMResult
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.runnables import RunnableLambda
# from langchain.output_parsers.json import SimpleJsonOutputParser

# # LangGraph imports
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages

# # Your existing imports
# from ..utils.promptManager import YAMLPromptManager
# from ..utils.structured_outputs import StockStruct, FinalStockStruct, OrderClassifier
# from ..utils.llm_tools import *

from ..utils.route_function import *
from ..utils.node_function import *

# load_dotenv()

# if not os.environ.get("GOOGLE_API_KEY"):
#     os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter API key for Google Gemini: ")


# class LoggingHandler(BaseCallbackHandler):
#     def on_chat_model_start(
#         self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs
#     ) -> None:
#         print("Chat model started")

#     def on_llm_end(self, response: LLMResult, **kwargs) -> None:
#         print(f"Chat model ended, response: {response}")

#     def on_chain_start(
#         self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs
#     ) -> None:
#         print(f"Chain {serialized.get('name')} started")

#     def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
#         print(f"Chain ended, outputs: {outputs}")


# State ì •ì˜
# class AdvisorState(TypedDict):
#     question: str
#     main_classification: dict
#     stock_classification: dict
#     route: str
#     final_result: Any
#     error: str


# callbacks = [LoggingHandler()]
# model = init_chat_model("gemini-2.5-flash", model_provider="google_genai")
# json_parser = SimpleJsonOutputParser()
# structured_llm = model.with_structured_output(FinalStockStruct)
# yaml_prompt_manager = YAMLPromptManager()


class LLMServiceGraph:
    def __init__(self):
        pass


    # def _format_stock_response(self, response):
    #     """ì£¼ì‹ ì‘ë‹µ í¬ë§·íŒ…"""
    #     return {
    #         **response,
    #         "type": "stock_advice",
    #         "category": "investment"
    #     }
    
    # def _format_general_response(self, response):
    #     """ì¼ë°˜ ì‘ë‹µ í¬ë§·íŒ…"""
    #     return {
    #         **response,
    #         "type": "general_advice",
    #         "category": "general"
    #     }
    
    # def _format_order_response(self, response):
    #     """ì£¼ë¬¸ ì‘ë‹µ í¬ë§·íŒ…"""
    #     return {
    #         **response,
    #         "type": "order_confirmation",
    #         "category": "transaction"
    #     }
    
    def _create_langgraph_chain(self):
        """LangGraph ì²´ì¸ ìƒì„±"""
        
        # # í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ë“¤
        # def stock_prompt(question: str):
        #     context = 'testì…ë‹ˆë‹¤'
        #     prompt = yaml_prompt_manager.create_chat_prompt("stock_advisor", context=context, question=question)
        #     return prompt

        # def general_prompt(question: str):
        #     context = 'testì…ë‹ˆë‹¤'
        #     prompt = yaml_prompt_manager.create_chat_prompt("general_advisor", context=context, question=question)
        #     return prompt

        # # ë¶„ë¥˜ê¸°ë“¤
        # classifier = yaml_prompt_manager.create_chat_prompt("stock_general_branch_prompt") | model
        # stock_classifier = yaml_prompt_manager.create_chat_prompt("stock_order_branch") | model.with_structured_output(OrderClassifier)

        # # ë…¸ë“œ í•¨ìˆ˜ë“¤
        # def classify_main(state: AdvisorState) -> AdvisorState:
        #     """1ì°¨ ë¶„ë¥˜: STOCK vs GENERAL"""
        #     try:
        #         question = state["question"]
        #         main_result = classifier.invoke({"question": question})
                
        #         is_stock = "STOCK" in main_result.content.upper()
        #         route = "STOCK" if is_stock else "GENERAL"
                
        #         return {
        #             **state,
        #             "main_classification": {"content": main_result.content, "is_stock": is_stock},
        #             "route": route
        #         }
        #     except Exception as e:
        #         return {**state, "error": str(e), "route": "ERROR"}

        # def classify_stock(state: AdvisorState) -> AdvisorState:
        #     """2ì°¨ ë¶„ë¥˜: STOCK_ORDER vs STOCK_GENERAL"""
        #     try:
        #         question = state["question"]
        #         stock_result = stock_classifier.invoke({"question": question})
                
        #         stock_type = stock_result.get("type", "").upper()
                
        #         return {
        #             **state,
        #             "stock_classification": stock_result,
        #             "route": stock_type
        #         }
        #     except Exception as e:
        #         return {**state, "error": str(e), "route": "ERROR"}

        # def process_stock_order(state: AdvisorState) -> AdvisorState:
        #     """STOCK_ORDER ì²˜ë¦¬"""
        #     try:
        #         classification = state["stock_classification"]
        #         # parsed_data = parse_stock_info(classification)
        #         # result = (structured_llm | order_stock | RunnableLambda(self._format_order_response)).invoke(classification)
        #         result = (structured_llm).invoke(classification)
        #         print(f"result : {result}")
                
        #         return {**state, "final_result": result,"type":"order_confirmation"}
        #     except Exception as e:
        #         return {**state, "error": str(e)}

        # def process_stock_general(state: AdvisorState) -> AdvisorState:
        #     """STOCK_GENERAL ì²˜ë¦¬"""
        #     try:
        #         question = state["question"]
        #         result = (stock_prompt(question) | model | json_parser | RunnableLambda(self._format_stock_response)).invoke({"question": question})

        #         return {**state, "final_result": result,"type":"stock_advice"}
        #     except Exception as e:
        #         return {**state, "error": str(e)}

        # def process_general(state: AdvisorState) -> AdvisorState:
        #     """GENERAL ì²˜ë¦¬"""
        #     try:
        #         question = state["question"]
        #         result = (general_prompt(question) | model | json_parser | RunnableLambda(self._format_general_response)).invoke({"question": question})

        #         return {**state, "final_result": result,"type":"general_advice"}
        #     except Exception as e:
        #         return {**state, "error": str(e)}

        # def handle_error(state: AdvisorState) -> AdvisorState:
        #     """ì—ëŸ¬ ì²˜ë¦¬"""
        #     error_result = {
        #         "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {state.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}",
        #         "type": "error"
        #     }
        #     return {**state, "final_result": error_result}

        # ë¼ìš°íŒ… í•¨ìˆ˜ë“¤
        # def route_after_main_classification(state: AdvisorState) -> Literal["classify_stock", "process_general", "handle_error"]:
        #     """ë©”ì¸ ë¶„ë¥˜ í›„ ë¼ìš°íŒ…"""
        #     route = state.get("route", "")
        #     if route == "ERROR":
        #         return "handle_error"
        #     elif route == "STOCK":
        #         return "classify_stock"
        #     else:
        #         return "process_general"

        # def route_after_stock_classification(state: AdvisorState) -> Literal["process_stock_order", "process_stock_general", "handle_error"]:
        #     """ì£¼ì‹ ë¶„ë¥˜ í›„ ë¼ìš°íŒ…"""
        #     route = state.get("route", "")
        #     if route == "ERROR":
        #         return "handle_error"
        #     elif route == "STOCK_ORDER":
        #         return "process_stock_order"
        #     else:
        #         return "process_stock_general"

        # ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(AdvisorState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("classify_main", classify_main)
        workflow.add_node("classify_stock", classify_stock)
        workflow.add_node("process_stock_order", process_stock_order)
        workflow.add_node("process_stock_general", process_stock_general)
        workflow.add_node("process_general", process_general)
        workflow.add_node("handle_error", handle_error)

        # ì—£ì§€ ì¶”ê°€
        workflow.add_edge(START, "classify_main")
        
        # ë©”ì¸ ë¶„ë¥˜ í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "classify_main",
            route_after_main_classification,
            {
                "classify_stock": "classify_stock",
                "process_general": "process_general",
                "handle_error": "handle_error"
            }
        )

        # ì£¼ì‹ ë¶„ë¥˜ í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "classify_stock",
            route_after_stock_classification,
            {
                "process_stock_order": "process_stock_order",
                "process_stock_general": "process_stock_general",
                "handle_error": "handle_error"
            }
        )

        # ëª¨ë“  ì²˜ë¦¬ ë…¸ë“œì—ì„œ ENDë¡œ
        workflow.add_edge("process_stock_order", END)
        workflow.add_edge("process_stock_general", END)
        workflow.add_edge("process_general", END)
        workflow.add_edge("handle_error", END)

        return workflow.compile()

    async def advisor_stream(self, question):
        """ìƒë‹´ ìŠ¤íŠ¸ë¦¼ (LangGraph ë²„ì „)"""
        try:
            # ë¨¼ì € ë¶„ë¥˜ ë©”ì‹œì§€ ì „ì†¡
            classification_data = {"content": f"ìƒë‹´ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n\n"}
            yield f"data: {json.dumps(classification_data, ensure_ascii=False)}\n\n"

            # LangGraph ì‹¤í–‰
            graph = self._create_langgraph_chain()
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤í–‰
            async for chunk in graph.astream(
                {"question": question}
                # , 
                # config={"callbacks": callbacks}
            ):
                # ê° ë…¸ë“œì˜ ì¶œë ¥ ì²˜ë¦¬
                for node_name, node_output in chunk.items():
                    print(f"Node {node_name} output: {node_output}")

                    if node_name == "classify_main":
                        route = node_output.get("route", "")
                        if route == "STOCK":
                            feedback = {"content": "ğŸ“ˆ ì£¼ì‹ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤...\n\n"}
                            yield f"data: {json.dumps(feedback, ensure_ascii=False)}\n\n"
                        elif route == "GENERAL":
                            feedback = {"content": "ğŸ’¬ ì¼ë°˜ ìƒë‹´ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤...\n\n"}
                            yield f"data: {json.dumps(feedback, ensure_ascii=False)}\n\n"

                    if node_name in ["process_stock_order", "process_stock_general", "process_general", "handle_error"]:

                        # ìµœì¢… ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ë§Œ ìŠ¤íŠ¸ë¦¬ë°
                        final_result = node_output.get("final_result")
                        print(f"Final result: {final_result}")
                        if final_result:

                            # íƒ€ì…ë³„ ì´ëª¨ì§€ ì¶”ê°€
                            type_emojis = {
                                "stock_advice": "ğŸ“Š ",
                                "general_advice": "ğŸ’¡ ",
                                "order_confirmation": "âœ… ",
                                "error": "âŒ "
                            }
                            prefix = type_emojis.get(final_result.get("type"), "")
                            for char in prefix:
                                yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"

                            # ê²°ê³¼ë¥¼ ì ì ˆíˆ ìŠ¤íŠ¸ë¦¬ë°
                            if isinstance(final_result, dict):
                                content = final_result.get("content", "")
                                if isinstance(content, dict):
                                    content_str = json.dumps(content, ensure_ascii=False)
                                    for char in content_str:
                                        yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
                                else:
                                    # ë¬¸ìë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
                                    for char in str(content):
                                        yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
                            else:
                                # ë¬¸ìë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
                                for char in str(final_result):
                                    yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
            
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            error_data = {"content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
