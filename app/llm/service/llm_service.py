    
import getpass
import os
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from ..utils.promptManager import YAMLPromptManager
from langchain_core.runnables import RunnableBranch, RunnablePassthrough
from langchain.output_parsers.json import SimpleJsonOutputParser
from ..utils.structured_outputs import StockStruct,FinalStockStruct,Joke
import json

load_dotenv()

if not os.environ.get("GOOGLE_API_KEY"):
  os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter API key for Google Gemini: ")


model = init_chat_model("gemini-2.5-flash", model_provider="google_genai")
json_parser = SimpleJsonOutputParser()
structured_llm = model.with_structured_output(Joke)
yaml_prompt_manager = YAMLPromptManager()

class LLMService:
    def __init__(self):
        pass
    
    def _create_routing_chain(self):
        """ë¼ìš°íŒ… ì²´ì¸ ìƒì„±"""
        
        # ê° ì–´ë“œë°”ì´ì € í”„ë¡¬í”„íŠ¸
        def stock_prompt(question: str) : 
            context = 'testì…ë‹ˆë‹¤'
            prompt = yaml_prompt_manager.create_chat_prompt("stock_advisor", context=context, question=question)
            return prompt


        # general_prompt = yaml_prompt_manager.create_chat_prompt("general_advisor")

        def general_prompt(question: str) :
            context = 'testì…ë‹ˆë‹¤'
            prompt = yaml_prompt_manager.create_chat_prompt("general_advisor", context=context, question=question)
            return prompt

        # ë¶„ë¥˜ê¸°
        classifier_prompt =  yaml_prompt_manager.create_chat_prompt("stock_general_branch_prompt")

        classifier = classifier_prompt | model
        

        # ğŸ¯ RunnableBranchë¡œ ë¶„ê¸°ì²˜ë¦¬
        routing_chain = RunnableBranch(
            # ì¡°ê±´1: STOCKì´ í¬í•¨ëœ ê²½ìš°
            (lambda x: "STOCK" in classifier.invoke({"question": x["question"]}).content.upper(),
             lambda x : stock_prompt(x["question"]) | structured_llm
            ),      # ì¡°ê±´2: ê¸°ë³¸ê°’ (GENERAL)
            lambda x : general_prompt(x["question"]) | model | json_parser
        )
        
        return routing_chain
    

    def advisor_stream(self, question):
        """ìƒë‹´ ìŠ¤íŠ¸ë¦¼"""
        try:
            # ë¨¼ì € ë¶„ë¥˜ ë©”ì‹œì§€ ì „ì†¡
            classification_data = {"content": f"ìƒë‹´ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n\n"}
            yield f"data: {json.dumps(classification_data, ensure_ascii=False)}\n\n"
            
            response = self._create_routing_chain().invoke({"question": question})
            print(f"ê²°ê³¼ê°’:{response}")

            # Chainìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            for chunk in self._create_routing_chain().stream({"question": question}):
                # contentê°€ ìˆìœ¼ë©´ í•œ ë²ˆì— ì²˜ë¦¬
                content = chunk.get("content", "") if isinstance(chunk, dict) else ""
                for char in content:
                    yield f"data: {json.dumps({'content': char}, ensure_ascii=False)}\n\n"
            
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            error_data = {"content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
    
